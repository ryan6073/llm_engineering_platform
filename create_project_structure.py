#!/usr/bin/env python
# -*- coding: UTF-8 -*-
"""
@Project : llm_engineering_platform (Creator Script)
@File : create_project_structure.py
@Author : Ryan Zhu (Generated by AI for Ryan Zhu)
@Date : 2025/05/19 10:46
"""

import os
import pathlib

# Script assumes it's run from the project's root directory (e.g., llm_engineering_platform/)
PROJECT_NAME_FOR_HEADER = "llm_engineering_platform"  # Used in file headers
AUTHOR = "Ryan Zhu"
FIXED_DATETIME = "2025/5/19 01:31"  # As per your original example for headers

PYTHON_FILE_HEADER_TEMPLATE = """#!/usr/bin/env python
# -*- coding: UTF-8 -*-
\"\"\"
@Project : {project_name}
@File : {file_name}
@Author : {author}
@Date : {date_str}
\"\"\"
"""

# Define the project structure (paths are relative to the project root)
# Tuple format: (path_str, type, optional_content_key)
# type: 'dir', 'py', 'empty_file', 'gitkeep_dir', 'content_file'
# optional_content_key: key to lookup in PREDEFINED_FILE_CONTENTS
STRUCTURE = [
    # src directory and its contents
    ("src", 'dir'),
    ("src/__init__.py", 'py'),

    ("src/prompts", 'dir'),
    ("src/prompts/__init__.py", 'py'),
    ("src/prompts/prompts.py", 'content_file', "src/prompts/prompts.py"),
    ("src/prompts/example_prompt.md", 'content_file', "src/prompts/example_prompt.md"),

    ("src/llms", 'dir'),
    ("src/llms/__init__.py", 'py'),
    ("src/llms/llm.py", 'content_file', "src/llms/llm.py"),

    ("src/config", 'dir'),
    ("src/config/__init__.py", 'py'),
    ("src/config/agents.py", 'content_file', "src/config/agents.py"),
    ("src/config/configuration.py", 'content_file', "src/config/configuration.py"),
    ("src/config/settings.py", 'py'),  # For Pydantic BaseSettings

    ("src/agents", 'dir'),
    ("src/agents/__init__.py", 'py'),
    ("src/agents/factory.py", 'content_file', "src/agents/factory.py"),
    ("src/agents/base_agent.py", 'py'),  # Optional base
    ("src/agents/specific_graph_agents.py", 'py'),  # For LangGraph agent examples

    ("src/orchestrator", 'dir'),
    ("src/orchestrator/__init__.py", 'py'),
    ("src/orchestrator/graph_manager.py", 'py'),  # For LangGraph management

    ("src/knowledge_base", 'dir'),
    ("src/knowledge_base/__init__.py", 'py'),
    ("src/knowledge_base/retrievers.py", 'py'),
    ("src/knowledge_base/loaders.py", 'py'),
    ("src/knowledge_base/schemas_registry.py", 'py'),

    ("src/tools", 'dir'),
    ("src/tools/__init__.py", 'py'),
    ("src/tools/base_tool.py", 'py'),
    ("src/tools/specific_tool_A.py", 'py'),

    ("src/models", 'dir'),  # Pydantic models
    ("src/models/__init__.py", 'py'),
    ("src/models/common.py", 'py'),
    ("src/models/agent_io.py", 'py'),  # For AgentState or similar
    ("src/models/knowledge.py", 'py'),

    ("src/utils", 'dir'),
    ("src/utils/__init__.py", 'py'),
    ("src/utils/helper.py", 'py'),

    # app directory (FastAPI/Web app)
    ("app", 'dir'),
    ("app/__init__.py", 'py'),
    ("app/main.py", 'py'),  # Main application entry point for FastAPI or other web server
    ("app/api", 'dir'),
    ("app/api/__init__.py", 'py'),
    ("app/api/deps.py", 'py'),  # Dependencies for FastAPI
    ("app/api/v1", 'dir'),
    ("app/api/v1/__init__.py", 'py'),
    ("app/api/v1/schemas.py", 'py'),  # Pydantic schemas for API requests/responses
    ("app/api/v1/endpoints", 'dir'),
    ("app/api/v1/endpoints/__init__.py", 'py'),
    ("app/api/v1/endpoints/prompt.py", 'py'),  # Example endpoint for prompt handling
    ("app/api/v1/endpoints/tasks.py", 'py'),  # Example endpoint for task management
    ("app/ws", 'dir'),  # For WebSocket connections if needed
    ("app/ws/__init__.py", 'py'),
    ("app/ws/connections.py", 'py'),

    # tests directory
    ("tests", 'dir'),
    ("tests/__init__.py", 'py'),
    ("tests/unit", 'dir'),
    ("tests/unit/__init__.py", 'py'),
    ("tests/unit/src", 'dir'),  # Mirror src structure for unit tests
    ("tests/unit/src/__init__.py", 'py'),
    ("tests/unit/src/prompts", 'dir'),
    ("tests/unit/src/prompts/__init__.py", 'py'),
    ("tests/unit/src/prompts/test_prompts.py", 'py'),
    ("tests/unit/src/agents", 'dir'),
    ("tests/unit/src/agents/__init__.py", 'py'),
    ("tests/unit/src/agents/test_factory.py", 'py'),
    # Add more test subdirectories mirroring src as needed
    ("tests/integration", 'dir'),
    ("tests/integration/__init__.py", 'py'),
    ("tests/integration/test_end_to_end.py", 'py'),

    # scripts directory
    ("scripts", 'dir'),
    ("scripts/__init__.py", 'py'),
    ("scripts/run_dev.sh", 'content_file', "scripts/run_dev.sh"),  # Example run script
    ("scripts/load_data.py", 'py'),  # Example script for data loading

    # data and logs directories
    ("data", 'dir'),
    ("data/knowledge_sources", 'gitkeep_dir'),  # For RAG documents
    ("data/processed", 'gitkeep_dir'),  # For processed data
    ("logs", 'gitkeep_dir'),  # For application logs

    # Root files
    (".env_template", 'content_file', ".env_template"),
    (".gitignore", 'content_file', ".gitignore"),
    ("Dockerfile", 'content_file', "Dockerfile"),
    ("docker-compose.yml", 'content_file', "docker-compose.yml"),
    ("requirements.txt", 'content_file', "requirements.txt"),  # uv can use this
    ("README.md", 'content_file', "README.md"),
    ("pyproject.toml", 'content_file', "pyproject.toml"),  # uv can use this (PEP 621)
]

PREDEFINED_FILE_CONTENTS = {
    "src/prompts/prompts.py": """\
# Copyright (c) 2025 Bytedance Ltd. and/or its affiliates
# SPDX-License-Identifier: MIT

import os
import dataclasses
from datetime import datetime
from jinja2 import Environment, FileSystemLoader, select_autoescape
from typing import List, Dict, Any

# Assuming AgentState might be a TypedDict or Pydantic model.
# For LangGraph, AgentState is often a TypedDict.
# from langgraph.prebuilt.chat_agent_executor import AgentState 
# If you define your own AgentState (e.g. in src/models/agent_io.py):
from src.models.agent_io import AgentState # Make sure this model is defined

from src.config.configuration import Configuration # Make sure this path is correct

# Initialize Jinja2 environment
# This expects .md template files to be in the same directory as this prompts.py file
PROMPTS_DIR = os.path.dirname(__file__)
env = Environment(
    loader=FileSystemLoader(PROMPTS_DIR),
    autoescape=select_autoescape(['html', 'xml', 'md']), # Be specific about autoescape
    trim_blocks=True,
    lstrip_blocks=True,
    keep_trailing_newline=True,
)


def get_prompt_template_string(prompt_name: str) -> str:
    \"\"\"
    Load and return a raw prompt template string using Jinja2.

    Args:
        prompt_name: Name of the prompt template file (without .md extension)

    Returns:
        The raw template string.
    \"\"\"
    try:
        # Jinja2's get_template returns a Template object. To get the string,
        # you'd typically read the file directly or render it.
        # For raw string, it's often easier to just read the file.
        template_path = os.path.join(PROMPTS_DIR, f"{prompt_name}.md")
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        raise ValueError(f"Error loading template string {prompt_name}: {e}")


def apply_prompt_template(
    prompt_name: str, state: AgentState, configurable: Configuration = None
) -> List[Dict[str, Any]]:
    \"\"\"
    Apply template variables to a prompt template and return formatted messages.

    Args:
        prompt_name: Name of the prompt template to use.
        state: Current agent state containing variables to substitute.
               Ensure 'messages' is a key in state if using AgentState directly.
        configurable: Optional configuration object.

    Returns:
        List of messages with the system prompt as the first message.
    \"\"\"
    current_state_dict = dict(state) if not isinstance(state, dict) else state

    state_vars = {
        "CURRENT_TIME": datetime.now().strftime("%a %b %d %Y %H:%M:%S %z"),
        **current_state_dict,
    }

    if configurable:
        # Ensure configurable is a dict or can be converted to one
        if dataclasses.is_dataclass(configurable):
            state_vars.update(dataclasses.asdict(configurable))
        elif isinstance(configurable, dict):
            state_vars.update(configurable)
        # else: log a warning or raise error if configurable is of unexpected type

    try:
        template = env.get_template(f"{prompt_name}.md")
        system_prompt_content = template.render(**state_vars)

        existing_messages = current_state_dict.get("messages", [])
        if not isinstance(existing_messages, list):
            # Log warning or handle error if messages format is unexpected
            existing_messages = []

        # The structure [{role: "system", content: ...}] + other_messages is common
        # for many LangChain/LangGraph chat models.
        return [{"role": "system", "content": system_prompt_content}] + existing_messages
    except Exception as e:
        # Consider more specific exception handling (e.g., jinja2.exceptions.TemplateNotFound)
        raise ValueError(f"Error applying template {prompt_name} with state {state_vars.keys()}: {e}")

""",
    "src/prompts/example_prompt.md": """\
You are a helpful and friendly AI assistant named "{{ agent_name | default('Assistant') }}".
Your goal is to assist the user with their tasks.
The current time is: {{ CURRENT_TIME }}.

{% if configurable and configurable.custom_instructions %}
Special Instructions for this session: {{ configurable.custom_instructions }}
{% endif %}

{% if messages and messages|length > 0 %}
Conversation History:
{% for message in messages %}
  {% if message.role == 'user' or message.role == 'human' %}
User: {{ message.content }}
  {% elif message.role == 'assistant' or message.role == 'ai' %}
Assistant: {{ message.content }}
    {% if message.tool_calls %}
Tool Calls:
      {% for tool_call in message.tool_calls %}
      - ID: {{ tool_call.id }}
        Tool: {{ tool_call.name }}
        Args: {{ tool_call.args }}
      {% endfor %}
    {% endif %}
  {% elif message.role == 'tool' %}
Tool Result (ID: {{ message.tool_call_id }}):
{{ message.content }}
  {% endif %}
{% endfor %}
{% else %}
This is the beginning of your conversation.
{% endif %}

Available tools:
{% if tools %}
  {% for tool in tools %}
  - {{ tool.name }}: {{ tool.description }}
    Args Schema: {{ tool.args_schema.schema() if tool.args_schema else 'N/A' }}
  {% endfor %}
{% else %}
You have no tools available for this task.
{% endif %}

Based on the conversation history and available tools, provide a thoughtful and accurate response.
If you need to use a tool, clearly state your intention and the tool you are using.
After receiving the tool's output, use it to formulate your final answer.
If the user's request is ambiguous, ask for clarification.
Think step-by-step.
""",
    "src/agents/factory.py": """\
from langgraph.prebuilt import create_react_agent # Standard ReAct agent factory
# For more complex agent types or custom graphs, you might use other LangGraph components
# from langgraph.graph import StateGraph, END

from src.prompts.prompts import apply_prompt_template 
from src.llms.llm import get_llm_by_type
from src.config.agents import AGENT_LLM_MAP
from src.models.agent_io import AgentState # Assuming AgentState is defined here

# Example: If you need a specific prompt template for ReAct
from langchain_core.prompts import ChatPromptTemplate


def create_llm_engineering_agent(agent_name: str, agent_type: str, tools: list, prompt_template_name: str):
    \"\"\"
    Factory function to create LangGraph agents.
    This example focuses on the ReAct agent, but can be extended.
    \"\"\"
    llm_config = AGENT_LLM_MAP.get(agent_type)
    if not llm_config:
        raise ValueError(f"LLM configuration for agent type '{agent_type}' not found in AGENT_LLM_MAP.")

    llm_instance = get_llm_by_type(llm_config)

    # For create_react_agent, the 'prompt' argument is typically a ChatPromptTemplate
    # that defines the system message. The ReAct logic (Thought, Action, Observation)
    # is handled by the graph structure itself.
    # Your `apply_prompt_template` function seems to generate a full message list
    # including the system prompt. This is more aligned with how a `ChatAgentExecutor`
    # or a custom graph node might consume prompts.

    # Option 1: Adapt apply_prompt_template to be used for the system message of ReAct
    # This means the Jinja template (e.g., example_prompt.md) should be designed
    # primarily as a system message for the ReAct agent.

    # A simplified system message for ReAct might look like:
    # react_system_message_template = "You are a ReAct agent. Your name is {agent_name}. Use tools to answer."
    # react_prompt = ChatPromptTemplate.from_messages([
    #     ("system", react_system_message_template.format(agent_name=agent_name))
    # ])
    # This is a very basic example. The prompt_template_name could point to a more complex system message.

    # Let's assume prompt_template_name refers to a Jinja template that, when rendered
    # with minimal state (or just global vars like agent_name), produces the system message content.

    # Minimal state for rendering a system message template
    # The `state` passed to `apply_prompt_template` by the lambda in the original user code
    # `lambda state: apply_prompt_template(prompt_template_name, state)`
    # might be problematic if `create_react_agent` doesn't provide a rich state object
    # for its `prompt` callable during its internal setup.
    # The `prompt` in `create_react_agent` is usually simpler.

    # For `create_react_agent`, the `messages_modifier` is often used to inject the system prompt.
    # Let's use `messages_modifier` as it's more idiomatic for `create_react_agent`.

    # The `apply_prompt_template` function expects an `AgentState`.
    # We need to construct a minimal or representative AgentState for the system prompt generation.
    # This is a bit of a workaround because the system prompt is usually static or has few variables.

    # Create a placeholder state for rendering the system prompt via apply_prompt_template
    # This assumes your prompt template for ReAct is designed to be the *system message* part.
    initial_system_prompt_state = AgentState(messages=[], intermediate_steps=[]) # Corrected key name

    # The apply_prompt_template returns a list of messages. We need the system message content.
    # This is a bit convoluted if the template is just for the system message.
    # A simpler Jinja template just for the system message string would be more direct.

    # Let's make the system message template rendering more direct:
    from src.prompts.prompts import env as jinja_env # Access Jinja environment directly
    try:
        system_template = jinja_env.get_template(f"{prompt_template_name}.md")
        # Pass variables relevant to the system message
        system_message_content = system_template.render(
            agent_name=agent_name,
            agent_type=agent_type,
            tools=tools # Making tools available to the system prompt template
        )
    except Exception as e:
        raise ValueError(f"Error rendering system prompt template {prompt_template_name} for ReAct agent: {e}")

    # The `create_react_agent` function returns a compiled LangGraph.
    agent_graph = create_react_agent(
        model=llm_instance,
        tools=tools,
        # The system message can be passed directly or via messages_modifier
        messages_modifier=system_message_content # Pass the rendered system message string
    )

    print(f"ReAct Agent graph '{agent_name}' (type: '{agent_type}') created using template '{prompt_template_name}'.")
    return agent_graph

# You might have other factory functions for different types of LangGraph agents
# e.g., a custom StateGraph based agent
# def create_custom_graph_agent(agent_name: str, ...):
#     workflow = StateGraph(AgentState)
#     # ... define nodes and edges ...
#     app = workflow.compile()
#     return app

""",
    "src/llms/llm.py": """\
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
# Add imports for other LLM providers as needed, e.g., from langchain_community.chat_models import ChatOllama
from src.config.settings import get_settings # Assuming settings.py handles API key loading

# Global settings instance
settings = get_settings()

def get_llm_by_type(llm_config: dict):
    \"\"\"
    Retrieves an LLM instance based on its type and configuration.
    `llm_config` is expected to be a dictionary like:
    {
        "provider": "openai",
        "model_name": "gpt-3.5-turbo",
        "temperature": 0.7,
        # API key might be handled globally via settings or passed explicitly
    }
    \"\"\"
    provider = llm_config.get("provider", "").lower()
    model_name = llm_config.get("model_name")
    temperature = llm_config.get("temperature", 0.7)
    max_tokens = llm_config.get("max_tokens", 1024) # Example of another common param

    print(f"Attempting to get LLM for provider: {provider}, model: {model_name}")

    if provider == "openai":
        api_key = settings.openai_api_key
        if not api_key:
            raise ValueError("OpenAI API key not found in settings.")
        return ChatOpenAI(
            model_name=model_name, 
            temperature=temperature, 
            max_tokens=max_tokens,
            openai_api_key=api_key
        )
    elif provider == "anthropic":
        api_key = settings.anthropic_api_key
        if not api_key:
            raise ValueError("Anthropic API key not found in settings.")
        return ChatAnthropic(
            model=model_name, # Anthropic uses 'model' not 'model_name'
            temperature=temperature, 
            max_tokens=max_tokens,
            anthropic_api_key=api_key
        )
    # Example for Ollama (local LLM)
    # elif provider == "ollama":
    #     # Ollama typically runs as a service; base_url might be configurable
    #     # from langchain_community.chat_models import ChatOllama
    #     return ChatOllama(
    #         model=model_name, 
    #         temperature=temperature
    #         # base_url=settings.ollama_base_url # If configurable
    #     )
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")
""",
    "src/config/agents.py": """\
# Example configuration for AGENT_LLM_MAP
# Maps agent types (or specific agent names) to LLM configurations 
# that src.llms.llm.get_llm_by_type can understand.

AGENT_LLM_MAP = {
    "default_react_agent": { # This is an agent_type key
        "provider": "openai",  # or "anthropic", "ollama", etc.
        "model_name": "gpt-3.5-turbo",
        "temperature": 0.7,
        "max_tokens": 1500,
    },
    "data_query_react_agent": {
        "provider": "openai",
        "model_name": "gpt-4o", 
        "temperature": 0.2,
        "max_tokens": 2000,
    },
    "creative_writing_agent": {
        "provider": "anthropic",
        "model_name": "claude-3-sonnet-20240229",
        "temperature": 0.8,
        "max_tokens": 3000,
    },
    # "ollama_mistral_agent": {
    #     "provider": "ollama", 
    #     "model_name": "mistral", 
    #     "temperature": 0.5,
    # }
    # The keys (e.g., "default_react_agent") should match the `agent_type`
    # parameter passed to your agent factory function.
}
""",
    "src/config/configuration.py": """\
from dataclasses import dataclass, field
from typing import Optional, Dict, Any

@dataclass
class Configuration:
    \"\"\"
    Global or configurable settings that can be passed to prompts or agents.
    These can be dynamically loaded or set.
    \"\"\"
    # Example fields:
    company_name: str = "DefaultLLMPlatformUser"
    user_role: str = "analyst"
    max_iterations: int = 15 # Max iterations for ReAct or other looped agents
    debug_mode: bool = False
    custom_instructions: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

    # Add fields as needed by your application
    # For example, if different agents need different base URLs for tools:
    # tool_server_url: str = "http://localhost:8081/tools"

# Example of how you might load this:
# def load_app_configuration() -> Configuration:
#     # Load from a file, environment variables, etc.
#     # For simplicity, returning a default instance here
#     return Configuration(company_name="MyOrganization", debug_mode=True)
""",
    "src/config/settings.py": """\
from pydantic_settings import BaseSettings, SettingsConfigDict
from typing import Optional

class AppSettings(BaseSettings):
    # LLM API Keys
    openai_api_key: Optional[str] = None
    anthropic_api_key: Optional[str] = None
    # Add other API keys as needed (e.g., cohere_api_key: Optional[str] = None)

    # Vector Database Configuration (Example for Qdrant)
    qdrant_url: Optional[str] = "http://localhost:6333"
    qdrant_api_key: Optional[str] = None # Optional, if Qdrant is secured

    # Application Settings
    log_level: str = "INFO"
    fastapi_port: int = 8000
    fastapi_host: str = "0.0.0.0"

    # Ollama settings (if using local LLMs via Ollama)
    # ollama_base_url: str = "http://localhost:11434"

    # Redis settings (if used for caching, message queues, etc.)
    # redis_host: str = "localhost"
    # redis_port: int = 6379
    # redis_db: int = 0
    # redis_password: Optional[str] = None


    # model_config allows loading from .env file
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding='utf-8', extra='ignore')

_settings_instance = None

def get_settings() -> AppSettings:
    global _settings_instance
    if _settings_instance is None:
        _settings_instance = AppSettings()
    return _settings_instance

# Example usage:
# settings = get_settings()
# print(settings.openai_api_key)
""",
    "src/models/agent_io.py": """\
from typing import TypedDict, List, Dict, Any, Sequence, Optional
from langchain_core.messages import BaseMessage # For LangGraph compatibility
from langchain_core.agents import AgentAction, AgentFinish # For ReAct intermediate steps

class AgentState(TypedDict, total=False):
    \"\"\"
    Represents the state of an agent in a LangGraph.
    This is a common structure, but can be customized.
    `total=False` means not all keys are required.
    \"\"\"
    # Input to the agent/graph
    input: Optional[str]

    # Messages in the conversation
    messages: Sequence[BaseMessage] # LangGraph uses BaseMessage for messages

    # Intermediate steps for ReAct or similar agents
    # Each step could be an AgentAction and its corresponding observation (tool output)
    intermediate_steps: Optional[List[tuple[AgentAction, Any]]] 

    # Final output of the agent
    agent_outcome: Optional[AgentFinish] # AgentFinish contains return_values (final answer)

    # Scratchpad or internal monologue for the agent
    agent_scratchpad: Optional[List[BaseMessage]] # Or str, depending on agent type

    # Current depth or iteration count, useful for loops
    iteration_count: Optional[int]

    # Custom fields for your specific application
    # e.g., user_id, session_id, specific data retrieved
    user_id: Optional[str]
    session_id: Optional[str]
    retrieved_context: Optional[str]
    error_message: Optional[str] # To store any errors encountered

    # You can add more specific fields based on your graph's needs
    # For example, if you have a planning step:
    # plan: Optional[List[str]]
    # current_task: Optional[str]
""",
    ".gitignore": """\
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/

# Translations
*.mo
*.pot

# Django stuff:
*.log
# local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
.python-version

# pipenv
Pipfile.lock

# poetry
poetry.lock

# pdm
pdm.lock
.pdm.toml
.pdm-python

# PEP 582; __pypackages__ directory specified by PEP 582.
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.env.*
!.env_template
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# PyDev project settings
.pydevproject

# PyCharm
.idea/
*.iml
shelf/

# VSCode
.vscode/

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Logs and data (if not versioned, otherwise remove from here)
logs/
# data/ # Uncomment if your data directory should be ignored
# *.sqlite
# *.sqlite3

# Ruff cache
.ruff_cache
.ipynb_checkpoints
""",
    "Dockerfile": """\
# Start with a Python base image (choose a specific version for reproducibility)
FROM python:3.10-slim

# Set environment variables for Python
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV PIP_NO_CACHE_DIR=off
ENV PIP_DISABLE_PIP_VERSION_CHECK=on
ENV PIP_DEFAULT_TIMEOUT=100

# Set work directory in the container
WORKDIR /app

# Install system dependencies if any are needed by your Python packages
# For example, if a library needs gcc or other build tools:
# RUN apt-get update && apt-get install -y --no-install-recommends \
#     build-essential \
#     curl \
#     && rm -rf /var/lib/apt/lists/*

# Copy only requirements.txt first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies
# If using uv, you might install it first and then use it to install requirements
# RUN pip install uv
# RUN uv pip install --no-cache --system -r requirements.txt
RUN pip install --no-cache-dir -r requirements.txt


# Copy the rest of the application code into the container
COPY . .

# Expose the port your application runs on (e.g., for FastAPI)
# Ensure this matches the port Uvicorn or your app server is configured to use
EXPOSE 8000 

# Command to run the application
# This will depend on how your application is started.
# For a FastAPI application in app/main.py with an app object named `app`:
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]

# If your main entry point is a script, e.g., src/main_script.py:
# CMD ["python", "src/main_script.py"]
""",
    "docker-compose.yml": """\
version: '3.8'

services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm_engineering_platform_app
    ports:
      - "${FASTAPI_PORT:-8000}:8000" # Use host port from .env or default to 8000
    volumes:
      - .:/app # Mount current directory to /app in container for development (live reload)
    env_file:
      - .env # Load environment variables from .env file
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload # For FastAPI with reload during dev
    depends_on:
      redis: # Example: if your app uses Redis
        condition: service_healthy # Wait for Redis to be healthy
      qdrant: # Example: if your app uses Qdrant
        condition: service_started # Or service_healthy if Qdrant has a health check

  redis: # Example Redis service for caching or message queue
    image: redis:7-alpine
    container_name: llm_platform_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  qdrant: # Example Qdrant vector database service
    image: qdrant/qdrant:v1.9.0 # Use a specific version
    container_name: llm_platform_qdrant
    ports:
      - "6333:6333" # gRPC port
      - "6334:6334" # REST API port
    volumes:
      - qdrant_storage:/qdrant/storage
    # environment: # Qdrant specific environment variables if needed
      # - QDRANT__SERVICE__API_KEY=${QDRANT_API_KEY} # If using API key for Qdrant

volumes: # Define named volumes for data persistence
  redis_data:
  qdrant_storage:

# To run: docker-compose up --build
# To stop: docker-compose down
# To stop and remove volumes: docker-compose down -v
""",
    "requirements.txt": """\
# Core Langchain and LangGraph
langchain>=0.1.0,<0.2.0
langgraph>=0.0.30,<0.1.0
langchain-core>=0.1.0,<0.2.0
langchain-text-splitters>=0.0.1,<0.1.0

# LLM Provider Libraries
langchain-openai>=0.1.0,<0.2.0
langchain-anthropic>=0.1.0,<0.2.0
# langchain-google-genai
# langchain-cohere
# langchain-community # For Ollama and other community integrations

# For Prompt Templating
Jinja2>=3.0.0,<4.0.0

# For Web API (FastAPI)
fastapi>=0.100.0,<0.112.0
uvicorn[standard]>=0.20.0,<0.30.0

# For Pydantic models and settings
pydantic>=2.0.0,<3.0.0
pydantic-settings>=2.0.0,<3.0.0

# For HTTP clients (often used by Langchain or for custom tool calls)
httpx>=0.24.0,<0.28.0
aiohttp>=3.8.0,<4.0.0 # If any library specifically needs it

# For environment variable management
python-dotenv>=1.0.0,<2.0.0

# Vector Store Clients (Uncomment and specify versions as needed)
# qdrant-client>=1.7.0,<2.0.0
# chromadb>=0.4.0,<0.5.0
# faiss-cpu # or faiss-gpu

# Testing Frameworks
pytest>=7.0.0,<9.0.0
pytest-asyncio>=0.20.0,<0.24.0 # For testing async code
# coverage # For test coverage reports

# Linters and Formatters (for development, often managed via pyproject.toml with tools like Ruff, Black)
# ruff
# black

# Optional: For data handling if your tools or agents need it
# pandas
# numpy

# Optional: For specific LangChain community features or tools
# beautifulsoup4 # For WebBaseLoader, etc.
# unstructured # For document loading
# tiktoken # For token counting with OpenAI models
""",
    "README.md": """\
# LLM Engineering Platform

A platform for building, deploying, and managing sophisticated LLM-powered agents and applications using LangGraph.

## Overview

This platform provides a structured approach to developing language agents by leveraging:
- **LangGraph:** For defining agent workflows as stateful graphs.
- **LangChain:** For core LLM interactions, tool usage, and component integrations.
- **Jinja2 Prompts:** For flexible and maintainable prompt engineering.
- **FastAPI:** For exposing agent functionalities via a robust API.
- **Docker:** For containerization and reproducible deployments.
- **uv:** For fast Python package installation and virtual environment management.

## Project Structure

- `src/`: Core application source code.
  - `agents/`: Agent definitions, LangGraph agent implementations (e.g., ReAct), and factory functions.
  - `config/`: Python-based configurations (LLM mappings, global settings, Pydantic settings).
  - `llms/`: Abstractions for interacting with different LLM providers (OpenAI, Anthropic, local models).
  - `prompts/`: Jinja2-based prompt templating. `.md` files in this directory are prompt templates.
  - `orchestrator/`: Logic for managing and executing LangGraph graphs.
  - `knowledge_base/`: Modules for Retrieval Augmented Generation (RAG) - retrievers, loaders.
  - `tools/`: Custom tools that agents can use.
  - `models/`: Pydantic data models for internal data structures, API schemas, and agent states.
  - `utils/`: Common utility functions.
- `app/`: FastAPI application for exposing the platform via an API.
  - `main.py`: FastAPI application entry point.
  - `api/`: API versioning and endpoint definitions.
- `tests/`: Unit and integration tests.
- `scripts/`: Helper scripts (e.g., development server startup, data loading).
- `data/`: For local data, knowledge sources (often gitignored in production).
- `logs/`: Application logs.
- `pyproject.toml`: Project metadata, dependencies (can be used by `uv`), and tool configurations.
- `requirements.txt`: Alternative dependency list (can be used by `uv`).
- `Dockerfile`, `docker-compose.yml`: For containerized deployment.
- `.env_template`, `.env`: Environment variable management.

## Getting Started

### Prerequisites

- Python 3.9+ (uv will manage this within its virtual environment if needed)
- **uv:** Install from [https://github.com/astral-sh/uv](https://github.com/astral-sh/uv)
- Docker and Docker Compose (recommended for easy setup of services like databases)
- An LLM API key (e.g., OpenAI, Anthropic)

### Setup Instructions with `uv`

1.  **Clone the Repository (if applicable):**
    ```bash
    # git clone <repository_url>
    # cd llm_engineering_platform
    ```

2.  **Create and Activate Virtual Environment using `uv`:**
    `uv` can create and manage virtual environments. It will create a `.venv` directory by default.
    ```bash
    uv venv # Creates a virtual environment
    source .venv/bin/activate # On macOS/Linux
    # .venv\Scripts\activate # On Windows
    ```
    Alternatively, many `uv` commands can be run without explicitly activating the venv if you prefix them with `uv run -- `.

3.  **Install Dependencies using `uv`:**
    You can install dependencies from `requirements.txt` or `pyproject.toml`.

    Using `requirements.txt`:
    ```bash
    uv pip install -r requirements.txt
    ```
    Or, if your `pyproject.toml` is set up with a PEP 621 `[project]` table (and optionally `[tool.uv.sources]` or `[tool.poetry.dependencies]`), `uv` can install from it:
    ```bash
    uv pip install . # Installs dependencies from pyproject.toml
    # To install dev dependencies as well (if defined in pyproject.toml, e.g., under [project.optional-dependencies] or [tool.poetry.group.dev.dependencies]):
    # uv pip install ".[dev]" 
    ```

4.  **Configure Environment Variables:**
    Copy the `.env_template` file to a new file named `.env` and update it with your actual API keys and configurations:
    ```bash
    cp .env_template .env
    ```
    Edit `.env` with your details (e.g., `OPENAI_API_KEY`).

### Running the Application

#### Option 1: Using Docker Compose (Recommended for Production-like Environment & Services)

This will build the Docker image and start the application along with any defined services (e.g., Redis, Qdrant).
```bash
docker-compose up --build
```
The API will typically be available at `http://localhost:8000` (or the port specified in your `.env` / `docker-compose.yml`). API documentation (Swagger UI) will be at `http://localhost:8000/docs`.

To stop the services:
```bash
docker-compose down
```

#### Option 2: Running Natively with Uvicorn (for FastAPI development, using `uv`)

Ensure your virtual environment (managed by `uv`) is activated or use `uv run`.
The `scripts/run_dev.sh` script can be adapted or you can run Uvicorn directly.

Using `uv run` (activates the environment implicitly for the command):
```bash
uv run uvicorn app.main:app --reload --host $(grep FASTAPI_HOST .env | cut -d '=' -f2 || echo "0.0.0.0") --port $(grep FASTAPI_PORT .env | cut -d '=' -f2 || echo "8000")
```
Or, after activating the venv (`source .venv/bin/activate`):
```bash
uvicorn app.main:app --reload --host $(grep FASTAPI_HOST .env | cut -d '=' -f2 || echo "0.0.0.0") --port $(grep FASTAPI_PORT .env | cut -d '=' -f2 || echo "8000")
```
The `--reload` flag enables auto-reloading on code changes.

## Development

### Prompt Engineering
- Prompt templates are located in `src/prompts/` as `.md` files using Jinja2 syntax.
- The `src/prompts/prompts.py` module provides utilities for loading and rendering these templates.

### Creating New Agents (LangGraph)
1.  Define the agent's state structure (e.g., in `src/models/agent_io.py`).
2.  Implement the agent's nodes (functions that perform actions or LLM calls).
3.  Construct the `StateGraph` in `src/orchestrator/` or a dedicated agent module.
4.  Define edges and conditional edges to control the flow.
5.  Compile the graph into a runnable application.
6.  Use the agent factory (`src/agents/factory.py`) or directly instantiate and use the compiled graph.

### Tools
- Define custom tools by inheriting from LangChain's `BaseTool` or using the `@tool` decorator.
- Place tool definitions in `src/tools/`.
- Make tools available to agents during their initialization.

### Testing
- Write unit tests for individual components in `tests/unit/`.
- Write integration tests for agent flows and API endpoints in `tests/integration/`.
- Run tests using Pytest (can be run with `uv run pytest`):
  ```bash
  uv run pytest
  ```
  Or with coverage:
  ```bash
  uv run pytest --cov=src tests/
  ```

### Linting and Formatting
This project can be configured to use tools like Ruff (which `uv` can also run).
```bash
# Example with Ruff using uv
uv run ruff check .
uv run ruff format .
```

## Contributing
(Add guidelines for contributing to the project if applicable)

## License
(Specify the license for your project, e.g., MIT, Apache 2.0)
This project is licensed under the MIT License - see the LICENSE file for details.
""",
    ".env_template": """\
# LLM API Keys (Update with your actual keys)
OPENAI_API_KEY=your_openai_api_key_here
# ANTHROPIC_API_KEY=your_anthropic_api_key_here
# COHERE_API_KEY=your_cohere_api_key_here
# GOOGLE_API_KEY= # For Google Generative AI

# Vector Database Configuration (Example for Qdrant)
# QDRANT_URL=http://localhost:6333
# QDRANT_API_KEY= # Optional, if Qdrant is secured

# Application Settings
LOG_LEVEL=INFO
FASTAPI_HOST=0.0.0.0
FASTAPI_PORT=8000

# Ollama settings (if using local LLMs via Ollama)
# OLLAMA_BASE_URL=http://localhost:11434

# Redis settings (if used for caching, message queues, etc.)
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0
# REDIS_PASSWORD=
# REDIS_URL=redis://${REDIS_USER:-}:${REDIS_PASSWORD:-}${REDIS_HOST:-localhost}:${REDIS_PORT:-6379}/${REDIS_DB:-0}


# Other service credentials or configurations
# MY_CUSTOM_SERVICE_URL=http://api.example.com
# MY_CUSTOM_SERVICE_API_KEY=your_custom_service_key
""",
    "pyproject.toml": """\
[tool.poetry] # Example if using Poetry. uv can also read this.
# If not using Poetry, you might simplify this to a PEP 621 [project] table
# which uv fully supports.
name = "llm-engineering-platform"
version = "0.1.0"
description = "A platform for building resilient language agents using LangGraph."
authors = ["Ryan Zhu <your.email@example.com>"] # Replace with actual email
readme = "README.md"
license = "MIT" # Or your chosen license
packages = [{include = "src"}, {include = "app"}]

[tool.poetry.dependencies]
python = "^3.9" # Specify your Python version compatibility
langchain = ">=0.1.0,<0.2.0"
langgraph = ">=0.0.30,<0.1.0"
langchain-core = ">=0.1.0,<0.2.0"
langchain-text-splitters = ">=0.0.1,<0.1.0"
langchain-openai = ">=0.1.0,<0.2.0"
langchain-anthropic = {version = ">=0.1.0,<0.2.0", optional = true}
# langchain-google-genai = {version = "...", optional = true}
# langchain-cohere = {version = "...", optional = true}
langchain-community = {version = ">=0.0.1,<0.1.0", optional = true} # For Ollama, etc.
jinja2 = ">=3.0.0,<4.0.0"
fastapi = ">=0.100.0,<0.112.0"
uvicorn = {extras = ["standard"], version = ">=0.20.0,<0.30.0"}
pydantic = ">=2.0.0,<3.0.0"
pydantic-settings = ">=2.0.0,<3.0.0"
httpx = ">=0.24.0,<0.28.0"
aiohttp = {version = ">=3.8.0,<4.0.0", optional = true}
python-dotenv = ">=1.0.0,<2.0.0"
# Vector Store Clients (mark as optional if not always needed)
qdrant-client = {version = ">=1.7.0,<2.0.0", optional = true}
# chromadb = {version = ">=0.4.0,<0.5.0", optional = true}
# faiss-cpu = {version = "...", optional = true} # or faiss-gpu
tiktoken = {version = ">=0.5.0,<0.8.0", optional = true}


[tool.poetry.group.dev.dependencies] # Development dependencies
pytest = ">=7.0.0,<9.0.0"
pytest-asyncio = ">=0.20.0,<0.24.0"
coverage = {extras = ["toml"], version = ">=7.0.0,<8.0.0"}
ruff = ">=0.1.0,<0.5.0" # Linter and formatter, uv can run ruff
# black = ">=23.0.0,<25.0.0" # Alternative formatter
pre-commit = ">=3.0.0,<4.0.0" # For git hooks

[tool.poetry.extras]
anthropic = ["langchain-anthropic"]
google = ["langchain-google-genai"]
cohere = ["langchain-cohere"]
community = ["langchain-community"]
qdrant = ["qdrant-client"]
chroma = ["chromadb"]
faiss = ["faiss-cpu"] # or "faiss-gpu"
openai_tiktoken = ["tiktoken"]
all_llms = ["langchain-anthropic", "langchain-google-genai", "langchain-cohere", "langchain-community"]
all_vectorstores = ["qdrant-client", "chromadb", "faiss-cpu"]


[build-system] # Needed for tools like pip and uv to build the project
requires = ["poetry-core>=1.0.0"] # If using Poetry as the build backend
build-backend = "poetry.core.masonry.api"
# If not using Poetry, you might use Hatchling, setuptools, or Flit.
# For a simple project primarily using requirements.txt, this section might be minimal
# or geared towards setuptools if you intend to build a wheel/sdist.
# uv itself does not require this section to install from requirements.txt or pyproject.toml (PEP 621).

[tool.ruff]
# See https://docs.astral.sh/ruff/configuration/ for all options
line-length = 88 # Or 100, 120, etc.
indent-width = 4
target-version = "py39" # Your minimum Python version

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # Pyflakes
    "I",  # isort
    "C",  # flake8-comprehensions
    "B",  # flake8-bugbear
    "UP", # pyupgrade
    "RUF", # Ruff-specific rules
]
ignore = [
    "E501", # Line too long (handled by formatter)
    "B008", # Do not perform function calls in argument defaults (sometimes intended)
]
# fixable = ["ALL"] # Auto-fix all possible issues
# unfixable = []

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-string-normalization = false
line-ending = "auto"

[tool.pytest.ini_options]
minversion = "7.0"
addopts = "-ra -q --cov=src --cov-report=term-missing --cov-report=xml" # Add options here
testpaths = [
    "tests/unit",
    "tests/integration",
]
python_files = "test_*.py"
python_classes = "Test*"
python_functions = "test_*"
asyncio_mode = "auto" # For pytest-asyncio

[tool.coverage.run]
source = ["src", "app"] # Include app directory in coverage if it has significant logic
omit = [
    "src/*/__init__.py", 
    "app/*/__init__.py",
    "src/config/settings.py", 
]

[tool.coverage.report]
fail_under = 70 # Example: fail if coverage is below 70%
show_missing = true
""",
    "scripts/run_dev.sh": """\
#!/bin/bash
# Development server startup script for FastAPI application, uv-aware

# Default values
HOST=${FASTAPI_HOST:-0.0.0.0}
PORT=${FASTAPI_PORT:-8000}
LOG_LEVEL=${LOG_LEVEL:-info}
RELOAD_FLAG="--reload"
USE_UV_RUN=true # Set to false to use manual venv activation

# Check for .env file and load it if it exists
if [ -f .env ]; then
  # Source .env to make variables available to this script
  set -o allexport
  source .env
  set +o allexport
fi

# Update HOST and PORT if they are set in .env, overriding defaults
HOST=${FASTAPI_HOST:-$HOST}
PORT=${FASTAPI_PORT:-$PORT}
LOG_LEVEL=${LOG_LEVEL:-$LOG_LEVEL}

echo "Starting FastAPI development server..."
echo "Host: $HOST"
echo "Port: $PORT"
echo "Log Level: $LOG_LEVEL"
echo "Reload: Enabled"

# Check if uv is installed
if ! command -v uv &> /dev/null
then
    echo "uv could not be found. Please install uv: https://github.com/astral-sh/uv"
    USE_UV_RUN=false # Fallback to standard venv if uv is not found
fi

if [ "$USE_UV_RUN" = true ] && command -v uv &> /dev/null; then
  echo "Using 'uv run' to start Uvicorn..."
  uv run uvicorn app.main:app --host "$HOST" --port "$PORT" --log-level "$LOG_LEVEL" $RELOAD_FLAG
else
  echo "Attempting to use standard virtual environment..."
  # Activate virtual environment if it exists and is not already active
  if [ -d ".venv" ] && [ -z "$VIRTUAL_ENV" ]; then
    echo "Activating virtual environment '.venv'..."
    source .venv/bin/activate
  elif [ -n "$VIRTUAL_ENV" ]; then
    echo "Already in a virtual environment: $VIRTUAL_ENV"
  else
    echo "Warning: Not in a virtual environment and '.venv' not found. Dependencies might not be correctly resolved."
  fi

  if ! command -v uvicorn &> /dev/null
  then
      echo "Uvicorn could not be found. Please ensure it is installed in your environment."
      echo "Try: uv pip install uvicorn[standard] or pip install uvicorn[standard]"
      exit 1
  fi
  uvicorn app.main:app --host "$HOST" --port "$PORT" --log-level "$LOG_LEVEL" $RELOAD_FLAG
fi
"""
}


def create_project_files():
    """Creates or updates the project structure in the current directory."""
    base_path = pathlib.Path(".")  # Current directory

    print(f"Updating/Creating project structure in {base_path.resolve()}")

    for item_config in STRUCTURE:
        item_path_str = item_config[0]
        item_type = item_config[1]
        content_key = item_config[2] if len(item_config) > 2 else None

        item_path = base_path / item_path_str

        item_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure parent directory exists

        if item_type == 'dir':
            item_path.mkdir(parents=True, exist_ok=True)
            # print(f"Ensured directory: {item_path}") # Less verbose
        elif item_type == 'gitkeep_dir':
            item_path.mkdir(parents=True, exist_ok=True)
            gitkeep_file = item_path / ".gitkeep"
            gitkeep_file.touch(exist_ok=True)
            # print(f"Ensured directory with .gitkeep: {item_path}")
        elif item_type == 'py':
            header_content = PYTHON_FILE_HEADER_TEMPLATE.format(
                project_name=PROJECT_NAME_FOR_HEADER,
                file_name=item_path.name,
                author=AUTHOR,
                date_str=FIXED_DATETIME
            )
            with open(item_path, 'w', encoding='utf-8') as f:
                f.write(header_content)
                if item_path.name != "__init__.py":
                    f.write("\n\npass # Placeholder\n")
            # print(f"Created/Replaced Python file: {item_path}")
        elif item_type == 'empty_file':
            item_path.touch(exist_ok=True)
            # print(f"Created/Touched empty file: {item_path}")
        elif item_type == 'content_file':
            content = PREDEFINED_FILE_CONTENTS.get(content_key, f"# Placeholder for {item_path.name}\n")

            is_python_file = item_path.suffix == '.py'
            is_init_file = item_path.name == "__init__.py"
            has_shebang = content.strip().startswith("#!/usr/bin/env python")
            has_copyright = any(
                line.strip().startswith("# Copyright") or line.strip().startswith("# SPDX-License-Identifier:")
                for line in content.splitlines()[:5]  # Check first 5 lines for copyright
            )

            if is_python_file and not is_init_file and not has_shebang and not has_copyright:
                header_content = PYTHON_FILE_HEADER_TEMPLATE.format(
                    project_name=PROJECT_NAME_FOR_HEADER,
                    file_name=item_path.name,
                    author=AUTHOR,
                    date_str=FIXED_DATETIME
                )
                content = header_content + "\n" + content

            with open(item_path, 'w', encoding='utf-8') as f:
                f.write(content)
            # print(f"Created/Replaced file with content: {item_path} (from key: {content_key})")

        if item_path.exists():
            print(f"Processed: {item_path}")
        else:
            print(f"Error creating: {item_path}")

    print(f"\nProject structure update/creation complete in {base_path.resolve()}.")
    print("Next steps could be:")
    print("1. Ensure 'uv' is installed (https://github.com/astral-sh/uv).")
    print(
        "2. Create and activate virtual environment: `uv venv` then `source .venv/bin/activate` (or equivalent for your OS).")
    print(
        "3. Install dependencies: `uv pip install -r requirements.txt` or `uv pip install .` (if using pyproject.toml).")
    print("4. Populate `.env` from `.env_template` with your API keys and configurations.")
    print("5. Initialize Git if not already: `git init`, `git add .`, `git commit -m \"Initial project structure\"`.")
    print("6. Start developing your LangGraph agents and application logic!")


if __name__ == "__main__":
    create_project_files()
